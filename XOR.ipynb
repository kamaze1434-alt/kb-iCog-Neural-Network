{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKsJ6kyj1NAwRW8+zh9s5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamaze1434-alt/kb-iCog-Neural-Network/blob/main/XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **1 ,Import Libraries**"
      ],
      "metadata": {
        "id": "f3hIyWzbRf7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCGDdY0wEVJD"
      },
      "outputs": [],
      "source": [
        "# 1,first import necessary libraries numpy and matplot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2, **XOR Dataset**"
      ],
      "metadata": {
        "id": "58RH4UK-RsGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2, Definne XOR Dataset\n",
        "#The XOR dataset consists of four input pairs and their corresponding exclusive-OR outputs.\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])#input\n",
        "y = np.array([[0], [1], [1], [0]])#output\n",
        "for i in y:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGBVVyBJtx6",
        "outputId": "a9b7c399-3015-4fe5-fcdc-ac6da097a327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[1]\n",
            "[1]\n",
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3, **Define activation functio**"
      ],
      "metadata": {
        "id": "GoGP1e4kSWWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Sigmoid function"
      ],
      "metadata": {
        "id": "o7LXy9UPSmLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reason\n",
        "'''\n",
        "For the XOR problem, the Sigmoid activation function is ideal for both the hidden and output layers\n",
        "because it is suitable for binary classification tasks and probability estimation.\n",
        "\n",
        " '''\n",
        "\n",
        " #Sigmoid: Maps inputs to a range between 0 and 1.\n",
        "#Sigmoid Derivative: Used during backpropagation to calculate gradients."
      ],
      "metadata": {
        "id": "Su84xOPWSVzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigoid(z):\n",
        "  return 1/(1 + np.exp(z))\n",
        "def sigmoid_derivative(a):\n",
        "  # 'a' is the output of the sigmoid function\n",
        "  return a*(1-a)"
      ],
      "metadata": {
        "id": "9IkVIV1WXGcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 **Create neural Network Class**"
      ],
      "metadata": {
        "id": "hvf0KskJZhob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using OOP principles help us organize components like layers, weights, and biases into reusable objects."
      ],
      "metadata": {
        "id": "PaUx81XqZH5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o1XDWgmdahYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 **Parameter Intialization**"
      ],
      "metadata": {
        "id": "AgI26-hjaJoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the network architecture (e.g., 2 input nodes, 4 hidden nodes, 1 output node) and assign random weights and zero biases\n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,layers,learning_rate=0.1):\n",
        "    self.learning_rate=learning_rate\n",
        "  # Initialize weights with small random values to prevent exploding gradients\n",
        "    self.weights = [np.random.randn(layers[i] , layers[i+1])*0.1\n",
        "                    for i in range(len(layers)-1)]\n",
        "    self.biases = [np.zero((1,layers[i+i])) for i in range(len(layers)-1)]\n"
      ],
      "metadata": {
        "id": "faGsyVy3aEGs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 **Forward propagation**"
      ],
      "metadata": {
        "id": "TyDpmPwzUyYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing the network output by applying linear transformations\n",
        "#(Z = W*X + b) and passing them through the sigmoid activation function layer by layer.\n",
        "\n",
        "def forward(self,X):\n",
        "  self.activations =[X]\n",
        "  for w,b in zip(self.weights, self.biases):\n",
        "    z= np.dot(self.activations[-1],w) +b\n",
        "    a = sigmoid(z)\n",
        "    self.activations.append(a)\n",
        "  return self.activations[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "9zhoH9bAU8WS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 **Backward prpagation**"
      ],
      "metadata": {
        "id": "0KwJGGARYrAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate how much each weight contributed to the error using the chain rule, moving backward from the output layer to the input.\n",
        "def backwadr(self,X,y):\n",
        "  #calcualting output layer eror\n",
        "  error= self.activation[-1]-y\n",
        "  delta = error*sigmoid_derivative(self.activation[-1])\n",
        "  #list to storage gradient\n",
        "  dW = []\n",
        "  db = []\n",
        "  # propagate backward error through layers\n",
        "  for i in range(len(self.weight),-1,-1,-1):\n",
        "    dW.insert(0, np.dot(self.activations[i].T, delta))\n",
        "    db.insert(0, np.sum(delta ,axis=0, keepdims=True))\n",
        "\n",
        "    if i>0:\n",
        "      error = np.dot(delta, self.weights[i].T )\n",
        "      delta = error.sigmoid(self.activations[i])\n",
        "\n",
        "  return dW,db\n"
      ],
      "metadata": {
        "id": "PxUwYsvsVgUl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4, **Paramete updates**\n"
      ],
      "metadata": {
        "id": "rQfvnZwQdVUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjust weights and biases using Gradient Descent to minimize the loss.\n",
        "def update_parameters(self,dW,db):\n",
        "  for i in range((self.weights,)):\n",
        "    self.weights[i]-= self.learning_rate*dW[i]\n",
        "    self.biases[i]-= self.biases*db[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4lzW-xXY-FF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5, **Excute the trainig loop**"
      ],
      "metadata": {
        "id": "m7vuaN13e6m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all steps into a loop to train the model over many epochs, tracking the loss to ensure it decreases monotonically.\n",
        "nn = NeuralNetwork([2,4,1], learning_rate=0.5)\n",
        "loss= []\n",
        "for epoch in range(1000):\n",
        "  #1,forward pass\n",
        "  prediction = nn.forward(X)\n",
        "\n",
        "  #2,compute MSE loss\n",
        "  loss = np.mean((prediction - y)**2)\n",
        "  losses.append(loss)\n",
        "\n",
        "  #3,backward pass\n",
        "  dW,db = nn.backward(X,y)\n",
        "\n",
        "  #4, update paramaters\n",
        "  nn.update_parameters(dW,db)\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"epoch {epoch},loss{loss.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# final prediction test\n",
        "print(\"\\n XOR prediction>\")\n",
        "print(nn.forwar(X))"
      ],
      "metadata": {
        "id": "oH_XIGQAfiG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 **Visualizing learning performanace**"
      ],
      "metadata": {
        "id": "pt2LcKlDkKAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"training performance curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "06PFMqzIkJYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFIvS-3jh_ws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}